{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK_p5Gg0Rt-P"
   },
   "source": [
    "# DECISION TREE \n",
    "\n",
    "#### Abhinav Bohra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1994 Census database\n",
    "#### Prediction task is to determine whether a person makes over 50K a year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43Q7DYasnCYP"
   },
   "source": [
    "# **Decision Tree Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79hmuTWHRt-X"
   },
   "outputs": [],
   "source": [
    "#Data Structure to store tree\n",
    "class tree_node:\n",
    "    def __init__(self, attribute, label, subAttributes):\n",
    "        self.attribute = attribute                                    #Node representing Question\n",
    "        self.label = label                                            #Label Class (<=50K or >50K)\n",
    "        self.subAttributes = subAttributes                            #Edges representing possible attribute values of node\n",
    "        self.child = []                                               #List storing Children nodes of main node\n",
    "        \n",
    "# Class containg Decision Tree and all related functions        \n",
    "class decisionTree:\n",
    "    \n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth if max_depth is not None else 10     #max_depth default value =10\n",
    "        self.max_depth = self.max_depth if max_depth != -1 else 100     #set max_depth value very high to allow full growth\n",
    "        self.root      = None\n",
    "        self.attributes = []\n",
    "        \n",
    "    #-------------------------------------------------------------------------------------------------       \n",
    "    #Function to compute the entropy for data-set\n",
    "    def findEntropy(self,countYes, countNo):\n",
    "        ratio_yes, ratio_no = countYes/(countYes + countNo), countNo/(countYes + countNo)\n",
    "        if not (ratio_yes and ratio_no):\n",
    "            return 0\n",
    "        else:\n",
    "            return -ratio_yes * np.log(ratio_yes) - ratio_no * np.log(ratio_no)\n",
    "    \n",
    "    #Function to find the Best Classifier Attribute\n",
    "    def findBestAttribute(self,trainX, trainY, attributes, entropy):\n",
    "        \n",
    "        gains = []\n",
    "        \n",
    "        for index in range(len(attributes)):\n",
    "            total_entropy , total_population = 0, 0        \n",
    "            for indexY in np.unique(trainX[:,index]):            \n",
    "                temp = np.where(trainX == indexY)[0]            \n",
    "                countYes = ( trainY[temp]== \">50K\"  ).sum()\n",
    "                countNo  = ( trainY[temp]== \"<=50K\" ).sum()\n",
    "                newEntropy = self.findEntropy(countYes, countNo)\n",
    "                total_entropy += temp.shape[0]*newEntropy\n",
    "                total_population += temp.shape[0]\n",
    " \n",
    "            total_entropy /= total_population\n",
    "            gains.append(entropy - total_entropy)\n",
    " \n",
    "        return gains.index(max(gains))\n",
    " \n",
    "    #ID3 Algorithm\n",
    "    def train(self,trainX, trainY, attributes,depth):\n",
    "        countYes = ( trainY == \">50K\"  ).sum()\n",
    "        countNo  = ( trainY == \"<=50K\" ).sum()\n",
    "        \n",
    "        if not countYes:\n",
    "            return tree_node(None, \"<=50K\", None)\n",
    " \n",
    "        elif not countNo:\n",
    "            return tree_node(None, \">50K\", None)\n",
    " \n",
    "        elif not attributes.shape[0] or depth >= self.max_depth:\n",
    "            if countYes > countNo:\n",
    "                return tree_node(None, \">50K\", None)\n",
    "            return tree_node(None, \"<=50K\", None)\n",
    "        \n",
    "        depth = depth + 1        \n",
    "        entropy = self.findEntropy(countYes, countNo)\n",
    "        bestIndex = self.findBestAttribute(trainX, trainY, attributes, entropy)\n",
    "        bestAttribute = attributes[bestIndex]\n",
    "        subAttributes = np.unique(trainX[:,bestIndex])\n",
    "        node = tree_node(bestAttribute, None, subAttributes)\n",
    "        \n",
    "        for attribute in subAttributes:\n",
    " \n",
    "            temp = np.where( trainX == attribute )[0]\n",
    "            newTrainX = np.delete(trainX[temp], bestIndex, 1)\n",
    "            newTrainY = trainY[temp]\n",
    "            newAttributes = np.delete(attributes, bestIndex)\n",
    "            node.child.append(self.train(newTrainX, newTrainY, newAttributes,depth))\n",
    " \n",
    "        return node    \n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------  \n",
    "    def fitTree(self,X_train,Y_train,attributes):\n",
    "       \n",
    "        self.attributes = attributes\n",
    "        \n",
    "        self.root = self.train(X_train,Y_train, np.array(self.attributes),0)\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------\n",
    "    #Function to classify datapoint into target class\n",
    "    def predict(self,x,node):\n",
    "        if not node.attribute:\n",
    "            return node.label\n",
    "        else:\n",
    "            ind   = np.where(node.attribute == np.array(self.attributes))[0]\n",
    "            edge  = x[ind]\n",
    "            index = np.where(edge[0] == np.array(node.subAttributes))[0]\n",
    "\n",
    "            if(len(index) == 0 ):\n",
    "                return \"<=50K\"         #this is the case when an edge was never seen in training data \n",
    "\n",
    "            nextNode = node.child[index[0]]\n",
    "            return self.predict(x,nextNode)\n",
    "\n",
    "    #Function to fetch predictions\n",
    "    def getPredictions(self,X_test):\n",
    "        preds=[]\n",
    "        for x in X_test:\n",
    "            preds.append( self.predict(x,self.root) )\n",
    "            \n",
    "        return preds\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------\n",
    "    #Function to compute accuracy\n",
    "    def getAccuracy(self,preds,Y_test):\n",
    "        numCorrect = 0\n",
    "        for i in range(0,len(Y_test)):\n",
    "            if(preds[i]==Y_test[i]):\n",
    "                numCorrect = numCorrect + 1\n",
    "\n",
    "        return(numCorrect/len(Y_test))\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZeAB-O-cVks"
   },
   "source": [
    "###  setData : **Function to Split Data into Train-set, Validation-set, Test-set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3EKFgpfcTaj"
   },
   "outputs": [],
   "source": [
    "def setData(data,target_var,splitRatio1=None,splitRatio2=None): \n",
    "    '''\n",
    "    Takes dataframe and target variable name as input and splits into X and Y sets using target_var\n",
    "    Futher splits X into X_train , X_CV, X_test\n",
    "    Futher splits Y into Y_train , Y_CV, Y_test\n",
    "    Splitting done as per input split ratios\n",
    "    Default Split Ration -> [train : CV : test] = [64 : 16 : 20]\n",
    "    return attributes (column name) of set X and X_train,X_test,X_CV,Y_train,Y_test,Y_CV as numpy array\n",
    "    '''\n",
    "    Y = data[target_var].reset_index(drop=True)\n",
    "    X =  data.copy(deep=True)\n",
    "    X.drop([target_var], axis='columns', inplace=True)\n",
    "    \n",
    "    if(X is None or Y is None):\n",
    "        print(\"Error : X or Y cannot be of NoneType. \")\n",
    "        return\n",
    "    \n",
    "    if(len(X)!= len(Y)):\n",
    "        print(\"Error : X and Y should be of same dimension. \")\n",
    "        return\n",
    "        \n",
    "    if(splitRatio1 is not None and (splitRatio1 <= 0 or splitRatio1>= 1) ):\n",
    "        print(\"Error : splitRatio1 should be between 0 and 1. \")\n",
    "        return\n",
    "                      \n",
    "    if(splitRatio2 is not None and (splitRatio2 <= 0 or splitRatio2>= 1) ):\n",
    "        print(\"Error : splitRatio2 should be between 0 and 1. \")\n",
    "        return\n",
    "            \n",
    "    splitRatio1 = splitRatio1 if splitRatio1 is not None else 0.64     #splitRatio1 default value = 0.64\n",
    "    splitRatio2 = splitRatio2 if splitRatio2 is not None else 0.16     #splitRatio1 default value = 0.16\n",
    "    \n",
    "        \n",
    "    #Split into trainset, cross validation set and testset        \n",
    "    \n",
    "    cutIndex1 = int (splitRatio1*len(X))\n",
    "    cutIndex2 = cutIndex1 + int (splitRatio2*len(X))\n",
    "\n",
    "    X_train = X[:cutIndex1]\n",
    "    X_CV    = X[cutIndex1: cutIndex2]\n",
    "    X_test  = X[cutIndex2:]\n",
    "\n",
    "    Y_train = Y[:cutIndex1]\n",
    "    Y_CV    = Y[cutIndex1: cutIndex2]\n",
    "    Y_test  = Y[cutIndex2:]\n",
    "\n",
    "    attributes = X_train.columns\n",
    "    return attributes,X_train.to_numpy(),  X_test.to_numpy(),  X_CV.to_numpy(),  Y_train.to_numpy(),  Y_test.to_numpy(),  Y_CV.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_W2pqMbce1X"
   },
   "source": [
    "## showTree:  **Function to print Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9H7qXw3mVGHV"
   },
   "outputs": [],
   "source": [
    "def showTree(root,level):\n",
    "    if not root.attribute:\n",
    "        print(root.label)\n",
    "    \n",
    "    for ind, child in enumerate(root.child):\n",
    "        if child.attribute:\n",
    "            print(\"\\t\"*level + root.attribute + \" = \" + root.subAttributes[ind] + \"\\n\" , end = '')     \n",
    "        else:\n",
    "            print(\"\\t\"*level + root.attribute + \" = \" + root.subAttributes[ind] + \":- \" , end = ' ')\n",
    "        \n",
    "        showTree(child,level+1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orCdIsEjgIbB"
   },
   "source": [
    "## pruneTree: **Function to Prune Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXJvjTUzRuBV"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "        Statsitical Test Used : Reduced Error Pruning\n",
    "\n",
    "'''\n",
    "def pruneTree(node,validationSet_X, validationSet_Y):  \n",
    "    #print(node.attribute)\n",
    "    if not node.attribute:    #if node is a label, pruning cannot be done, return\n",
    "        return\n",
    "    \n",
    "    if len(validationSet_X) == 0:\n",
    "        return\n",
    "    \n",
    "    #Check accuracy before pruning\n",
    "    preds_before_pruning    = myDT.getPredictions(validationSet_X)\n",
    "    accuracy_before_pruning = myDT.getAccuracy(preds_before_pruning,validationSet_Y)\n",
    "    \n",
    "    #Store node data temporarily\n",
    "    temp_node       = tree_node(\"none\",\"none\",\"none\")\n",
    "    temp_node.attribute=node.attribute\n",
    "    temp_node.label=node.label\n",
    "    temp_node.child = node.child\n",
    "    temp_node.subAttributes=node.subAttributes\n",
    "    \n",
    "    #Prune node\n",
    "    node.attribute=None\n",
    "    node.subAttributes=None\n",
    "    node.child=[]\n",
    "    \n",
    "    countYes = ( validationSet_X == \">50K\"  ).sum()\n",
    "    countNo  = ( validationSet_Y == \"<=50K\" ).sum()\n",
    "\n",
    "    if countYes > countNo:\n",
    "        node.label=\">50K\"\n",
    "    else:\n",
    "        node.label=\"<=50K\"\n",
    "       \n",
    "    #Check accuracy after pruning\n",
    "    \n",
    "    preds_after_pruning    = myDT.getPredictions(validationSet_X)\n",
    "    accuracy_after_pruning = myDT.getAccuracy(preds_after_pruning,validationSet_Y)\n",
    "    \n",
    "    if accuracy_after_pruning >= accuracy_before_pruning:\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "      #Copy initial node\n",
    "        node.label         = temp_node.label\n",
    "        node.attribute      = temp_node.attribute\n",
    "        node.subAttributes = temp_node.subAttributes\n",
    "        node.child         = temp_node.child\n",
    "\n",
    "        for child in node.child:\n",
    "            pruneTree(child,validationSet_X, validationSet_Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbU74uZEd4A3"
   },
   "source": [
    "## fetchData:  **Function to fetch data from directory and perform data pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EI8uySrmRt-2"
   },
   "outputs": [],
   "source": [
    "def fetchData():\n",
    "    #Setting up Training Dataset\n",
    "    data = pd.read_csv('dataset.data', sep=\", \", header=None,engine='python')\n",
    "    data.columns =['age','workclass','fnlwgt','education','education-num','marital-status','occupation',\n",
    "    'relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','target']\n",
    "\n",
    "    #Data Preprocessing\n",
    "    def fillMissingVal(x,mode):\n",
    "        if x == '?':\n",
    "            return mode\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def splitVar(x,threshold):\n",
    "        if x <= threshold:\n",
    "            return \"<=\"+ str(threshold)\n",
    "        else:\n",
    "            return \">\" + str(threshold)\n",
    "\n",
    "    #Fill Missing Values    \n",
    "    for col in data.keys()[:-1]:\n",
    "        mode = (data[col].mode()[0]) \n",
    "        data[col]=data.apply(lambda x: fillMissingVal(x[col],mode),axis=1)\n",
    "            \n",
    "    #Convert Continous Varaibles into Discrete Variables using split threshold    \n",
    "    age_threshold=37\n",
    "    data['age']=data.apply(lambda x: splitVar(x['age'],age_threshold),axis=1)\n",
    "    \n",
    "    fnlwgt_threshold=178300\n",
    "    data['fnlwgt']=data.apply(lambda x: splitVar(x['fnlwgt'],fnlwgt_threshold),axis=1)\n",
    "\n",
    "    # Dropping few variables after data analysis\n",
    "    data.drop(['capital-gain', 'capital-loss','hours-per-week','education-num'], axis='columns', inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSE4huo3RuAF"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PART 1\n",
    "\n",
    "Build a decision tree by taking as input a maximum depth and by randomly splitting the dataset as 80/20 split\n",
    "and provide the accuracy by averaging over 10 random 80/20 splits. Consider that particular tree which provides \n",
    "the best test accuracy as the desired one. \n",
    "\n",
    "'''\n",
    "\n",
    "def findBestTree(maxDepth):\n",
    "    accSum=0\n",
    "    max=0\n",
    "    best_Dataset=data\n",
    "    for i in range (0,10):\n",
    "        temp_data=data.sample(frac=1)\n",
    "        #Instantiate Decision Tree with input depth\n",
    "        myDT  = decisionTree(maxDepth)\n",
    "        #Set Training, crossvalidation and  Testing Data\n",
    "        attributes,X_train,X_test,X_CV,Y_train,Y_test,Y_CV = setData(temp_data,'target')\n",
    "        #Fit Tree to training data\n",
    "        myDT.fitTree(X_train,Y_train,attributes)\n",
    "        #Fetch predictions\n",
    "        preds = myDT.getPredictions(X_test)\n",
    "        #Calculate Accuracy\n",
    "        acc   = myDT.getAccuracy(preds,Y_test)   \n",
    "        #Destroy Instance\n",
    "        del myDT     \n",
    "        #Store Best Dataset split\n",
    "        accSum = accSum + acc\n",
    "        if(acc > max):\n",
    "            max = acc\n",
    "            best_Dataset=temp_data\n",
    "\n",
    "    #Calculate Average Accuracy     \n",
    "    accAvg =accSum/10\n",
    "    return accAvg, max, best_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hU-0ifXuRuBB"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PART 2 : Best possible depth limit to be used for your dataset \n",
    "'''\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def findBestDepth(bestDataset):\n",
    "    AccuracyTrain =[]\n",
    "    AccuracyTest  =[]\n",
    "    x=[]\n",
    "    maxD=0\n",
    "    maxAcc =0\n",
    "    for depth in range (1,11):\n",
    "        #Fetch Dataset\n",
    "        fetchData()\n",
    "        #Instantiate Decision Tree with input depth\n",
    "        myDT1  = decisionTree(depth)\n",
    "        #Set Training, crossvalidation and  Testing Data\n",
    "        attributes,X_train,X_test,X_CV,Y_train,Y_test,Y_CV = setData(best_Dataset,'target')\n",
    "        #Fit Tree to training data\n",
    "        myDT1.fitTree(X_train,Y_train,attributes)\n",
    "        \n",
    "        preds = myDT1.getPredictions(X_train)\n",
    "        acctrain   = myDT1.getAccuracy(preds,Y_train)\n",
    "        AccuracyTrain.append(acctrain)\n",
    "        \n",
    "        preds1    = myDT1.getPredictions(X_test)\n",
    "        acctest  = myDT1.getAccuracy(preds1,Y_test)\n",
    "        AccuracyTest.append(acctest)\n",
    "        \n",
    "        if(maxAcc < acctest):\n",
    "            maxAcc = acctest\n",
    "            maxD   = depth\n",
    "\n",
    "        x.append(depth)\n",
    "\n",
    "    plt.plot(x, AccuracyTrain,label=\"Train\")\n",
    "    plt.plot(x, AccuracyTest ,label=\"Test\" ) \n",
    "    plt.legend()\n",
    "    plt.xlabel('Depth') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.title('Decision Tree Depth Analysis') \n",
    "    plt.show()  \n",
    "    return maxD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Driver Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tv2xsMcRt_W"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Fetch Dataset\n",
    "data = fetchData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "9r0fF2W2RuAb",
    "outputId": "f855f720-c2f2-4ffa-e60d-8dbccbbf92d7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "      PART 1 : Provide the accuracy by averaging over 10 random splits. Consider that particular tree \n",
    "      which provides the best test accuracy as the desired one. \n",
    "\n",
    "      avgAcc stores the accuracy by averaging over 10 random dataset splits\n",
    "      maxAcc stores the max accuracy among all the 10 splits\n",
    "      best_Dataset stores the dataset which gave maximum accuracy (Tree for this dataset is built again in later steps)\n",
    "'''\n",
    "\n",
    "max_Depth = 3 \n",
    "# Change this variable to change input maxnimum depth\n",
    "# Enter -1 to allow full growth of tree\n",
    "\n",
    "avgAcc, maxAcc, best_Dataset = findBestTree(max_Depth)\n",
    "print(\"Maximum Accuracy is \" + str(maxAcc))\n",
    "print(\"Average Accuracy is \" + str(avgAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "uqZXms2QRuBO",
    "outputId": "2b017014-1aa4-46b3-f98f-d900b41b85f9"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "      PART 2 :  Find Best possible depth limit to be used for your dataset. \n",
    "      Provide a plot explaining the same.\n",
    "\n",
    "      Function findBestDepth makes tree with maxDepth varying from 1 to 10 using the best_Dataset \n",
    "      obtained from previous steps\n",
    "'''\n",
    "\n",
    "maxD = findBestDepth(best_Dataset)   \n",
    "print(\"Maximium Depth is : \" +str(maxD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 of assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Q25lJouMEuCN",
    "outputId": "d8e0ee94-b09a-4b0a-b7d7-d885c662af96"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  PART 3: Perform the pruning operation over the tree obtained in question 2 using a valid statistical test for comparison. \n",
    "  \n",
    "  Function Prune Tree perfoms reduced error post pruning on tree myDT obtained from PART 2\n",
    "  myDT gets pruned and updated accordingly\n",
    "'''\n",
    "#Build tree with best max depth and best Dataset split\n",
    "myDT  = decisionTree(maxD)\n",
    "#Set Training, crossvalidation and  Testing Data\n",
    "attributes,X_train,X_test,X_CV,Y_train,Y_test,Y_CV = setData(best_Dataset,'target')\n",
    "#Fit Tree to training data\n",
    "myDT.fitTree(X_train,Y_train,attributes)\n",
    "\n",
    "preds = myDT.getPredictions(X_test)\n",
    "acc_before_pruning   = myDT.getAccuracy(preds,Y_test)\n",
    "        \n",
    "#Prune Tree\n",
    "pruneTree(myDT.root,X_CV,Y_CV)\n",
    "\n",
    "preds_new = myDT.getPredictions(X_test)\n",
    "acc_after_pruning  = myDT.getAccuracy(preds_new,Y_test)\n",
    "\n",
    "print(\"Accuracy before pruning : \" + str(acc_before_pruning))\n",
    "print(\"Accuracy after pruning : \"  + str(acc_after_pruning ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 of assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1PN26Zx3Hmxf",
    "outputId": "7f6bd60a-9f2b-4fe5-d3bc-29b559fb8839"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  PART 4 : Print the final decision tree obtained from question 3 following the hierarchical levels of data attributes as nodes of the tree\n",
    "\n",
    "  Function showTree prints the pruned tree myDT obtained from PART 3 in the desired format\n",
    "'''\n",
    "\n",
    "#Print Pruned Decision Tree\n",
    "print(\"Final Tree after Post Pruning\")\n",
    "showTree(myDT.root,0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Group66_DT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
